Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{sohldickstein2015deep,
      title={Deep Unsupervised Learning using Nonequilibrium Thermodynamics}, 
      author={Jascha Sohl-Dickstein and Eric A. Weiss and Niru Maheswaranathan and Surya Ganguli},
      year={2015},
      eprint={1503.03585},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ho2020denoising,
      title={Denoising Diffusion Probabilistic Models}, 
      author={Jonathan Ho and Ajay Jain and Pieter Abbeel},
      year={2020},
      eprint={2006.11239},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@misc{kodali2017convergence,
      title={On Convergence and Stability of GANs}, 
      author={Naveen Kodali and Jacob Abernethy and James Hays and Zsolt Kira},
      year={2017},
      eprint={1705.07215},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}


@misc{miyato2018spectral,
      title={Spectral Normalization for Generative Adversarial Networks}, 
      author={Takeru Miyato and Toshiki Kataoka and Masanori Koyama and Yuichi Yoshida},
      year={2018},
      eprint={1802.05957},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{thanhtung2020catastrophic,
      title={On Catastrophic Forgetting and Mode Collapse in Generative Adversarial Networks}, 
      author={Hoang Thanh-Tung and Truyen Tran},
      year={2020},
      eprint={1807.04015},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{arjovsky2017wasserstein,
      title={Wasserstein GAN}, 
      author={Martin Arjovsky and Soumith Chintala and Léon Bottou},
      year={2017},
      eprint={1701.07875},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{metz2017unrolled,
      title={Unrolled Generative Adversarial Networks}, 
      author={Luke Metz and Ben Poole and David Pfau and Jascha Sohl-Dickstein},
      year={2017},
      eprint={1611.02163},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{rombach2022highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2022},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ruiz2023dreambooth,
      title={DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation}, 
      author={Nataniel Ruiz and Yuanzhen Li and Varun Jampani and Yael Pritch and Michael Rubinstein and Kfir Aberman},
      year={2023},
      eprint={2208.12242},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{10.5555/3104482.3104568,
author = {Welling, Max and Teh, Yee Whye},
title = {Bayesian learning via stochastic gradient langevin dynamics},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a "sampling threshold" and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {681–688},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}

@misc{nichol2021improved,
      title={Improved Denoising Diffusion Probabilistic Models}, 
      author={Alex Nichol and Prafulla Dhariwal},
      year={2021},
      eprint={2102.09672},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Kingma_2019,
   title={An Introduction to Variational Autoencoders},
   volume={12},
   ISSN={1935-8245},
   url={http://dx.doi.org/10.1561/2200000056},
   DOI={10.1561/2200000056},
   number={4},
   journal={Foundations and Trends® in Machine Learning},
   publisher={Now Publishers},
   author={Kingma, Diederik P. and Welling, Max},
   year={2019},
   pages={307–392} }


@misc{oord2018neural,
      title={Neural Discrete Representation Learning}, 
      author={Aaron van den Oord and Oriol Vinyals and Koray Kavukcuoglu},
      year={2018},
      eprint={1711.00937},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{li2020faceshifter,
      title={FaceShifter: Towards High Fidelity And Occlusion Aware Face Swapping}, 
      author={Lingzhi Li and Jianmin Bao and Hao Yang and Dong Chen and Fang Wen},
      year={2020},
      eprint={1912.13457},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}